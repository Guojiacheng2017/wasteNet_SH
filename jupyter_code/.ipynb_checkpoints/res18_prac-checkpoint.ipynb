{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/vlfl8bvn5mqgcfzprpqf_zph0000gn/T/ipykernel_12672/2397816995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ResNet --jupyter version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# created by Jiacheng Guo at Dec 4 15:22:58 CST 2021\n",
    "# ResNet --jupyter version\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride = 1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel,outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            #shortcut，这里为了跟2个卷积层的结果结构一致，要做处理\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        body = self.block(X)\n",
    "#         print(body.shape, self.shortcut(X).shape)\n",
    "        body = body + self.shortcut(X)\n",
    "        body = F.relu(body)\n",
    "        return body\n",
    "        \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResBlock, num_classes=4):\n",
    "        super(ResNet, self).__init__()\n",
    "        # img.shape = 1 here\n",
    "        self.inchannel = 32\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResBlock, 32, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResBlock, 64, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResBlock, 128, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResBlock, 256, 2, stride=2)\n",
    "        self.avgPool = nn.AvgPool2d(4)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.conv(X)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgPool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    ## used to read binary files since our data files are in binary format\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def get_correct_and_accuracy(y_pred, y):\n",
    "    # y_pred is the nxC prediction scores\n",
    "    # give the number of correct and the accuracy\n",
    "    n = y.shape[0]\n",
    "    # find the prediction class label\n",
    "    _ ,pred_class = y_pred.max(dim=1)\n",
    "    correct = (pred_class == y).sum().item()\n",
    "    return correct ,correct/n\n",
    "\n",
    "## loading data from binary data files\n",
    "batch_1_dictionary = unpickle('cifar-10-data/data_batch_1')\n",
    "batch_2_dictionary = unpickle('cifar-10-data/data_batch_2')\n",
    "\n",
    "## get training, validation and testing sets\n",
    "X_train_all = np.array(batch_1_dictionary[b'data']).reshape(10000,3,32,32)  # 3072 = 3 channels x 32 width x 32 length\n",
    "y_train_all = np.array(batch_1_dictionary[b'labels'])\n",
    "validation_count = 1000\n",
    "train_count = X_train_all.shape[0] - validation_count  # 9000\n",
    "# print(\"y_train_all: \", y_train_all)\n",
    "X_train = X_train_all[:train_count] # head 9000\n",
    "y_train = y_train_all[:train_count]\n",
    "X_val = X_train_all[train_count:]  # tail 1000\n",
    "y_val = y_train_all[train_count:]\n",
    "X_test = np.array(batch_2_dictionary[b'data']).reshape(10000,3,32,32) # convert test set into secondary matrix\n",
    "y_test = np.array(batch_2_dictionary[b'labels'])\n",
    "\n",
    "\n",
    "# for RGB data we can simply divide by 255\n",
    "X_train_normalized = X_train / 255\n",
    "X_val_normalized = X_val / 255\n",
    "X_test_normalized = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_iteration = 20\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "\n",
    "resNet = ResNet(ResBlock, num_classes=10)\n",
    "print(\"model structure:\", resNet)\n",
    "#\n",
    "optimizer = optim.Adam(resNet.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_data = X_train_normalized.shape[0]\n",
    "n_batch = int(np.ceil(n_data/batch_size))\n",
    "\n",
    "# convert X_train and X_val to tensor\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)\n",
    "\n",
    "# convert training label to tensor and to type long\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "y_val_tensor = torch.tensor(y_val).long()\n",
    "\n",
    "print('X train tensor shape:', X_train_tensor.shape)\n",
    "print('n_batch: ', n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start \n",
    "train_loss_list = np.zeros(n_iteration)\n",
    "train_accu_list = np.zeros(n_iteration)\n",
    "val_loss_list = np.zeros(n_iteration)\n",
    "val_accu_list = np.zeros(n_iteration)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(n_iteration):\n",
    "    # first get a minibatch of data\n",
    "    train_loss = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for j in range(n_batch):\n",
    "#         print(\"\\nbatch\", j, \":\")\n",
    "#         start_time = time.time()\n",
    "        \n",
    "        batch_start_index = j*batch_size\n",
    "        # get data batch from the normalized data\n",
    "        X_batch = X_train_tensor[batch_start_index:batch_start_index+batch_size]\n",
    "        # get ground truth label y\n",
    "        y_batch = y_train_tensor[batch_start_index:batch_start_index+batch_size]\n",
    "\n",
    "#         print(X_batch.shape)\n",
    "        train_pred = resNet(X_batch)\n",
    "#         print(train_pred.shape)\n",
    "        train_crt, train_accu = get_correct_and_accuracy(train_pred, y_batch)\n",
    "        train_loss_i = criterion(train_pred, y_batch)\n",
    "        \n",
    "#         train_loss.append(train_loss_i)\n",
    "        train_loss.append(train_loss_i.detach().numpy())\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss_i.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(\"batch\", j, \":\\t\", time.time() - start_time)\n",
    "\n",
    "    # \n",
    "    val_pred = resNet(X_val_tensor)\n",
    "    val_crt, val_accu = get_correct_and_accuracy(val_pred, y_val_tensor)\n",
    "    val_loss = criterion(val_pred, y_val_tensor)\n",
    "    \n",
    "    ave_train_loss = np.sum(train_loss)/len(train_loss)\n",
    "        \n",
    "    print(\"Iter %d ,Train loss: %.3f, Train acc: %.3f, Val loss: %.3f, Val acc: %.3f\" \n",
    "          %(i ,ave_train_loss, train_accu, val_loss, val_accu)) \n",
    "    ## add to the logs so that we can use them later for plotting\n",
    "    train_loss_list[i] = ave_train_loss\n",
    "    train_accu_list[i] = train_accu\n",
    "    val_loss_list[i] = val_loss\n",
    "    val_accu_list[i] = val_accu\n",
    "    print(\"iteration\", i, \":\\t\", time.time() - start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
